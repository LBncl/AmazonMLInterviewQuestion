{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NAFLD.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Pm6KDT1PjNKB",
        "AjLiQ0hJjR3a",
        "nhKAAVJErIz-",
        "gfAnUz9My8oK",
        "spD21lRw-xjx",
        "I3JKdx8O4p5E",
        "HBvKTEZ5UUZ0",
        "9liHjz7i9nf0",
        "dI43cWj4blmP",
        "-PBHB5Ohbsb4",
        "Eak0dOKfeRe7"
      ],
      "authorship_tag": "ABX9TyN4V3mNMRHRH6ENq3InyDPs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LBncl/AmazonMLInterviewQuestion/blob/main/NAFLD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NAFLD PROJECT"
      ],
      "metadata": {
        "id": "JJ8ZGc1YjFZB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "Pm6KDT1PjNKB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import missingno as msno\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.model_selection import train_test_split\n",
        "import imblearn\n",
        "import collections\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import datetime\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from numpy.ma.core import mean\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from numpy import random"
      ],
      "metadata": {
        "id": "JxvMeYS9iIA3"
      },
      "execution_count": 234,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Cleaning"
      ],
      "metadata": {
        "id": "AjLiQ0hJjR3a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FcqiBWkdhpRx"
      },
      "outputs": [],
      "source": [
        "# Read in data\n",
        "df_raw = pd.read_excel('master_with_nordic_and_multiBM.xlsx')\n",
        "\n",
        "# Check if dataFrame is empty\n",
        "if df_raw.empty:\n",
        "    print('DataFrame is empty!')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Subset data frame\n",
        "main_df = df_raw.filter(items=['CPH_EV_AGE_CALC', 'TBL.PATIENT.INFO..PI_BL_GENDER', 'CPH_EV_CI_BMI_CALC', \n",
        "                         'TBL.ALL.EVENTS..AE_SF_ALCO_XS', 'insulin_resistance', 'hypertensive', 'waist_to_hip_ratio',\n",
        "                         'idf_metabolic_syndrome', 'eGFR', 'dyslipidaemia', 'fibroscan_stiffness_reliable',\n",
        "                         'TBL.ALL.EVENTS..AE_BR_ALT_iuL',\n",
        "                         'TBL.ALL.EVENTS..AE_BR_AST_iuL', 'TBL.ALL.EVENTS..AE_BR_GGT_iuL',\n",
        "                         'TBL.ALL.EVENTS..AE_BR_FERR_ugL',\n",
        "                         'TBL.ALL.EVENTS..AE_BR_PLT_109L', 'TBL.ALL.EVENTS..AE_BR_CREAT_umolL_CALC',\n",
        "                         'TBL.ALL.EVENTS..AE_BR_STG_mmolL_CALC',\n",
        "                         'TBL.ALL.EVENTS..AE_BR_ALBU_gL_CALC', 'TBL.ALL.EVENTS..AE_BR_BILI_umolL_CALC',\n",
        "                         'TBL.ALL.EVENTS..AE_BR_IGA',\n",
        "                         'TBL.ALL.EVENTS..AE_CD_OSA', 'LIT_NB_CK18_M30', 'LIT_NB_CK18_M65', 'LIT_NB_PRO_C3', 'LIT_NB_PRO_C6',\n",
        "                          'LIT_NB_ELF', 'FIB4', 'NFS', 'APRI', 'ADAPT', 'FIBC3', 'ABC3D', 'BARD', 'AST_ALT_Ratio', 'response_3b','CPH_EV_EVENT_TYPE'])"
      ],
      "metadata": {
        "id": "1s4uX2A1i5_V"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_df.describe()"
      ],
      "metadata": {
        "id": "XAdTEnHfK_oa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_df.dtypes"
      ],
      "metadata": {
        "id": "npDox8CZF7ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "msno.bar(main_df)"
      ],
      "metadata": {
        "id": "xfNW-jzTjEQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove duplicates\n",
        "print(main_df.shape)\n",
        "main_df = main_df.drop_duplicates()\n",
        "# Reset the index and drop original index column \n",
        "main_df = main_df.reset_index(drop=True)\n",
        "print(main_df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlSt0PZasBlg",
        "outputId": "f0550803-02bb-4315-b39a-cf4f2cd1dd6a"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(14236, 37)\n",
            "(13772, 37)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# View effect of changes\n",
        "main_df"
      ],
      "metadata": {
        "id": "bUF3N_7wGsai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert required variables to boolean \n",
        "features_raw = main_df\n",
        "\n",
        "# Loop to check for boolean columns\n",
        "for column in features_raw.iloc[:, 0:36]:\n",
        "  if features_raw[column].max() == 1 and features_raw[column].min() == 0:\n",
        "    features_raw[column] = features_raw[column].astype(bool)"
      ],
      "metadata": {
        "id": "QCXqyL6Rj0Cq"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_raw.dtypes"
      ],
      "metadata": {
        "id": "gLPRAVzQGHnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_raw"
      ],
      "metadata": {
        "id": "LlWcWs-dNSqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter to only select baseline event types\n",
        "print(main_df.shape)\n",
        "features_raw['CPH_EV_EVENT_TYPE'] = features_raw['CPH_EV_EVENT_TYPE'].astype(str)\n",
        "features_raw = features_raw.loc[features_raw['CPH_EV_EVENT_TYPE'] == 'Baseline']\n",
        "# Reset the index and drop original index column \n",
        "features_raw = features_raw.reset_index(drop=True)\n",
        "print(features_raw.shape)"
      ],
      "metadata": {
        "id": "nA-yM30qLk1F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c31ec6b-9f10-4c38-99e5-f52a0f817436"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(13772, 37)\n",
            "(8991, 37)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# View effect of changes\n",
        "features_raw"
      ],
      "metadata": {
        "id": "TedwMvISSD-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove baseline column\n",
        "features_raw = features_raw.iloc[:, 0:36]"
      ],
      "metadata": {
        "id": "ioyKwAjBOBTr"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove rows where there is no response value and set value to Bool\n",
        "print(features_raw.shape)\n",
        "print(features_raw['response_3b'].isna().sum())\n",
        "features_raw = features_raw[features_raw['response_3b'].notna()]\n",
        "print(features_raw.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYmwQ2ExKiki",
        "outputId": "0315ca6c-a380-43a1-98da-72e57f636815"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8991, 37)\n",
            "0\n",
            "(8991, 37)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features_baseline = features_raw"
      ],
      "metadata": {
        "id": "f4v9IEKB341n"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_baseline.plot(kind=\"box\", subplots=True, layout=(9,4), figsize=(50,50))"
      ],
      "metadata": {
        "id": "hGZ1cCN5CFeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_baseline.dtypes"
      ],
      "metadata": {
        "id": "6r24Gl-Q6aQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols_normalise = features_baseline.select_dtypes(include=[np.float64])\n",
        "\n",
        "# Normalise Data\n",
        "scale = StandardScaler()\n",
        "features_baseline[cols_normalise.columns] = scale.fit_transform(cols_normalise)"
      ],
      "metadata": {
        "id": "8MZJ7whZ0FFh"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset the index and drop original index column \n",
        "features_baseline = features_baseline.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "om2CqQCUDW9s"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View dataframe to confirm all changes\n",
        "features_baseline"
      ],
      "metadata": {
        "id": "leJRSFoL-kEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Imputation Strategies"
      ],
      "metadata": {
        "id": "nhKAAVJErIz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Single imputation with mean to replace not a number (NaNs)\n",
        "def nan2mean(fdf):\n",
        "    cols = list(fdf.columns)\n",
        "    imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "    fdf=imp.fit_transform(fdf)\n",
        "    fdf = pd.DataFrame(fdf, columns=cols)\n",
        "    return fdf"
      ],
      "metadata": {
        "id": "XdhqTVx5rXQ-"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Single imputation with median to replace not a number (NaNs)\n",
        "def nan2median(fdf):\n",
        "    cols = list(fdf.columns)\n",
        "    imp = SimpleImputer(missing_values=np.nan, strategy='median')\n",
        "    fdf=imp.fit_transform(fdf)\n",
        "    fdf = pd.DataFrame(fdf, columns=cols)\n",
        "    return fdf"
      ],
      "metadata": {
        "id": "k-PByFl_rexy"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Single imputation with most_frequent to replace not a number (NaNs)\n",
        "def nan2most_frequent(fdf):\n",
        "    cols = list(fdf.columns)\n",
        "    imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
        "    fdf=imp.fit_transform(fdf)\n",
        "    fdf = pd.DataFrame(fdf, columns=cols)\n",
        "    return fdf"
      ],
      "metadata": {
        "id": "a6mf5HU_rhvF"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Multiple Imputation by chained equation linear regression\n",
        "def nan2mice(fdf):\n",
        "    cols = list(fdf.columns)\n",
        "    lr = LinearRegression()\n",
        "    imp = IterativeImputer(estimator=lr,missing_values=np.nan, max_iter=50, imputation_order='roman',random_state=0)\n",
        "    fdf=imp.fit_transform(fdf)\n",
        "    fdf = pd.DataFrame(fdf, columns=cols)\n",
        "    return fdf"
      ],
      "metadata": {
        "id": "vy61XAe0r76R"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imputation by K neasrest neighbours\n",
        "def nan2knn(fdf):\n",
        "    cols = list(fdf.columns)\n",
        "    imp = KNNImputer(n_neighbors=2, weights=\"distance\")\n",
        "    fdf=imp.fit_transform(fdf)\n",
        "    fdf = pd.DataFrame(fdf, columns=cols)\n",
        "    return fdf"
      ],
      "metadata": {
        "id": "3OCLAwGRuUyu"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## New Imputation Strategy"
      ],
      "metadata": {
        "id": "fzUaEDdhi1ZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stub dataframe to store new baseline rows\n",
        "stub_df = pd.DataFrame(columns=['CPH_EV_AGE_CALC', 'TBL.PATIENT.INFO..PI_BL_GENDER', 'CPH_EV_CI_BMI_CALC', \n",
        "                         'TBL.ALL.EVENTS..AE_SF_ALCO_XS', 'insulin_resistance', 'hypertensive', 'waist_to_hip_ratio',\n",
        "                         'idf_metabolic_syndrome', 'eGFR', 'dyslipidaemia', 'fibroscan_stiffness_reliable',\n",
        "                         'TBL.ALL.EVENTS..AE_BR_ALT_iuL',\n",
        "                         'TBL.ALL.EVENTS..AE_BR_AST_iuL', 'TBL.ALL.EVENTS..AE_BR_GGT_iuL',\n",
        "                         'TBL.ALL.EVENTS..AE_BR_FERR_ugL',\n",
        "                         'TBL.ALL.EVENTS..AE_BR_PLT_109L', 'TBL.ALL.EVENTS..AE_BR_CREAT_umolL_CALC',\n",
        "                         'TBL.ALL.EVENTS..AE_BR_STG_mmolL_CALC',\n",
        "                         'TBL.ALL.EVENTS..AE_BR_ALBU_gL_CALC', 'TBL.ALL.EVENTS..AE_BR_BILI_umolL_CALC',\n",
        "                         'TBL.ALL.EVENTS..AE_BR_IGA',\n",
        "                         'TBL.ALL.EVENTS..AE_CD_OSA', 'LIT_NB_CK18_M30', 'LIT_NB_CK18_M65', 'LIT_NB_PRO_C3', 'LIT_NB_PRO_C6',\n",
        "                          'LIT_NB_ELF', 'FIB4', 'NFS', 'APRI', 'ADAPT', 'FIBC3', 'ABC3D', 'BARD', 'AST_ALT_Ratio', 'response_3b'])"
      ],
      "metadata": {
        "id": "NLEvIsI3MKZQ"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Subset data frame\n",
        "main_df_new = df_raw.filter(items=['CPH_EV_AGE_CALC', 'TBL.PATIENT.INFO..PI_BL_GENDER', 'CPH_EV_CI_BMI_CALC', \n",
        "                         'TBL.ALL.EVENTS..AE_SF_ALCO_XS', 'insulin_resistance', 'hypertensive', 'waist_to_hip_ratio',\n",
        "                         'idf_metabolic_syndrome', 'eGFR', 'dyslipidaemia', 'fibroscan_stiffness_reliable',\n",
        "                         'TBL.ALL.EVENTS..AE_BR_ALT_iuL',\n",
        "                         'TBL.ALL.EVENTS..AE_BR_AST_iuL', 'TBL.ALL.EVENTS..AE_BR_GGT_iuL',\n",
        "                         'TBL.ALL.EVENTS..AE_BR_FERR_ugL',\n",
        "                         'TBL.ALL.EVENTS..AE_BR_PLT_109L', 'TBL.ALL.EVENTS..AE_BR_CREAT_umolL_CALC',\n",
        "                         'TBL.ALL.EVENTS..AE_BR_STG_mmolL_CALC',\n",
        "                         'TBL.ALL.EVENTS..AE_BR_ALBU_gL_CALC', 'TBL.ALL.EVENTS..AE_BR_BILI_umolL_CALC',\n",
        "                         'TBL.ALL.EVENTS..AE_BR_IGA',\n",
        "                         'TBL.ALL.EVENTS..AE_CD_OSA', 'LIT_NB_CK18_M30', 'LIT_NB_CK18_M65', 'LIT_NB_PRO_C3', 'LIT_NB_PRO_C6',\n",
        "                          'LIT_NB_ELF', 'FIB4', 'NFS', 'APRI', 'ADAPT', 'FIBC3', 'ABC3D', 'BARD', 'AST_ALT_Ratio', 'response_3b','CPH_EV_EVENT_TYPE', 'SPIC',\n",
        "                          'CPH_ADM_BIOPSY_EVENT_DATE'])\n",
        "\n",
        "# Remove duplicates\n",
        "print(main_df_new.shape)\n",
        "main_df_new = main_df_new.drop_duplicates()\n",
        "# Reset the index and drop original index column \n",
        "main_df_new = main_df_new.reset_index(drop=True)\n",
        "print(main_df_new.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9QrJqeIKQ5N",
        "outputId": "7fba01df-b6d7-43d4-d11f-4b86225ae717"
      },
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(14236, 39)\n",
            "(14084, 39)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to string for comparrison\n",
        "main_df_new['CPH_EV_EVENT_TYPE'] = main_df_new['CPH_EV_EVENT_TYPE'].astype(str)\n",
        "\n",
        "# Drop observations with no date\n",
        "main_df_new = main_df_new[main_df_new['CPH_ADM_BIOPSY_EVENT_DATE'].notna()]\n",
        "print(main_df_new.shape)\n",
        "# Reset the index and drop original index column \n",
        "main_df_new = main_df_new.reset_index(drop=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-izm4WuMmh1",
        "outputId": "13752c97-09db-46d5-9478-26cb6110909b"
      },
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(13586, 39)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Group data by paitnet ID\n",
        "main_df_grouped = main_df_new.groupby('SPIC')\n",
        "# Loop over the agregated data\n",
        "for group_name, df_group in main_df_grouped:\n",
        "  # Sort the group based upon date\n",
        "  df_group = df_group.sort_values(by=['CPH_ADM_BIOPSY_EVENT_DATE'])\n",
        "  # Select the baseline observations\n",
        "  baseline_row = df_group.loc[df_group['CPH_EV_EVENT_TYPE'] == \"Baseline\"]\n",
        "  # Remove duplicate rows\n",
        "  baseline_row = baseline_row.drop_duplicates()\n",
        "  # Reset the index and drop original index column \n",
        "  baseline_row = baseline_row.reset_index(drop=True)\n",
        "  # If baseline row is non existant tell user\n",
        "  if baseline_row.empty:\n",
        "    print('No baseline for paitent: ' + df_group['SPIC'].item())\n",
        "  # If only baseline add basleine row to dataframe  \n",
        "  elif df_group.shape[0] <= 1:\n",
        "    stub_df = pd.concat([stub_df, baseline_row.iloc[:, 0:36]], ignore_index=True)\n",
        "  # Only select data to impute that has multiple observations\n",
        "  elif df_group.shape[0] > 1 :\n",
        "    # Selects date of baseline observation\n",
        "    start_date = baseline_row['CPH_ADM_BIOPSY_EVENT_DATE']\n",
        "    # Convert from series to datetime\n",
        "    start_date = datetime.datetime(start_date.dt.year,start_date.dt.month, start_date.dt.day)\n",
        "    # Calculate absoulte time differenance\n",
        "    df_group['Time Difference'] = abs(df_group['CPH_ADM_BIOPSY_EVENT_DATE'] - start_date)\n",
        "    # Remove all rows where time difference is to large to impute\n",
        "    df_group = df_group.loc[df_group['Time Difference'] < datetime.timedelta(days=200)]\n",
        "    # Check again to see if any data is imputeable\n",
        "    if df_group.shape[0] > 1:\n",
        "      # Select relevant data to impute\n",
        "      baseline_x = baseline_row.iloc[:, 0:36]\n",
        "      # for each column in the baseline row \n",
        "      for (columnName, columnData) in baseline_x.iteritems():\n",
        "        # Select value of current observation\n",
        "        val = columnData.item()\n",
        "        # If value is NaN\n",
        "        if np.isnan(val):\n",
        "          # For all other observations of the same paitent\n",
        "          for i in range(df_group.shape[0]-1):\n",
        "            # Select next observation \n",
        "            next = df_group.iloc[i+1]\n",
        "            new_val = next[columnName].item()\n",
        "            # If next value is NaN move on\n",
        "            columnData.item = new_val\n",
        "  # Select new imputed basline\n",
        "  baseline_row = df_group.loc[df_group['CPH_EV_EVENT_TYPE'] == \"Baseline\"]\n",
        "  stub_df = pd.concat([stub_df, baseline_row.iloc[:, 0:36]], ignore_index=True)\n",
        "  \n",
        "# Reomve duplicates\n",
        "stub_df = stub_df.drop_duplicates()"
      ],
      "metadata": {
        "id": "rSiYl7UyZhz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove rows where there is no response value\n",
        "print(stub_df.shape)\n",
        "stub_df = stub_df[stub_df['response_3b'].notna()]\n",
        "# Reset the index and drop original index column \n",
        "stub_df = stub_df.reset_index(drop=True)\n",
        "print(stub_df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zc5IR39rVddx",
        "outputId": "96d8058b-5382-4c06-98ac-e2bf59cf1cad"
      },
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8567, 36)\n",
            "(6956, 36)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop uneeded columns\n",
        "features_baseline_new_imputed = stub_df.iloc[:, 0:36]"
      ],
      "metadata": {
        "id": "1BNLeZAWYnSS"
      },
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop to check for boolean columns\n",
        "for column in features_baseline_new_imputed.iloc[:, 0:36]:\n",
        "  if features_baseline_new_imputed[column].max() == 1 and features_baseline_new_imputed[column].min() == 0:\n",
        "    features_baseline_new_imputed[column] = features_baseline_new_imputed[column].astype(bool)"
      ],
      "metadata": {
        "id": "49hb4d0_ami8"
      },
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols_normalise = features_baseline_new_imputed.select_dtypes(include=[np.float64])\n",
        "\n",
        "# Normalise Data\n",
        "scale = StandardScaler()\n",
        "features_baseline_new_imputed[cols_normalise.columns] = scale.fit_transform(cols_normalise)"
      ],
      "metadata": {
        "id": "eh5LPc6Fa0gq"
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset the index and drop original index column \n",
        "features_baseline_new_imputed = features_baseline_new_imputed.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "c6onpVaCa9Tb"
      },
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View changes to dataframe\n",
        "features_baseline_new_imputed"
      ],
      "metadata": {
        "id": "lRfsDVqnb6df"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imputation Methods"
      ],
      "metadata": {
        "id": "gfAnUz9My8oK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create new imputed datasets\n",
        "featuresKNN = nan2knn(features_baseline.iloc[:, 0:35])\n",
        "featuresMean = nan2mean(features_baseline.iloc[:, 0:35])\n",
        "featuresMedian = nan2median(features_baseline.iloc[:, 0:35])\n",
        "featuresMICE = nan2mice(features_baseline.iloc[:, 0:35])\n",
        "featuresMICE2 = nan2mice(features_baseline_new_imputed.iloc[:, 0:35])\n",
        "featuresMostFrq = nan2most_frequent(features_baseline.iloc[:, 0:35])\n",
        "featuresMostFrq = featuresMostFrq.apply(pd.to_numeric)\n",
        "\n",
        "# Create list of all imputation strategies\n",
        "imputed_features = [featuresKNN,featuresMean,featuresMedian,featuresMICE,featuresMostFrq]\n",
        "imputed_features_names = ['featuresKNN','featuresMean','featuresMedian','featuresMICE','featuresMostFrq']"
      ],
      "metadata": {
        "id": "d-pBTzt7UmCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the response value to the new imputed dataset\n",
        "for df in imputed_features:\n",
        "  df['response_3b'] = features_baseline['response_3b']"
      ],
      "metadata": {
        "id": "0INKMsbdPITj"
      },
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View a dataframe to make sure the response has been added\n",
        "featuresMICE"
      ],
      "metadata": {
        "id": "PSsckRcl8y2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the response to new method imputed data\n",
        "featuresMICE2['response_3b'] = features_baseline_new_imputed['response_3b']"
      ],
      "metadata": {
        "id": "fLo_Ih6BdATN"
      },
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View a dataframe to make sure the response has been added\n",
        "featuresMICE2"
      ],
      "metadata": {
        "id": "6Xa8Xl5sdq11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Update list of all imputation strategies\n",
        "imputed_features = [featuresKNN,featuresMean,featuresMedian,featuresMICE,featuresMostFrq, featuresMICE2]\n",
        "imputed_features_names = ['featuresKNN','featuresMean','featuresMedian','featuresMICE','featuresMostFrq', 'featuresMICE2']"
      ],
      "metadata": {
        "id": "RPm0X1l6gaqA"
      },
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plots to show how the inputed data denstiy compares to the raw data\n",
        "column_name = \"CPH_EV_AGE_CALC\"\n",
        "\n",
        "d = {'raw': features_baseline[column_name], \n",
        "     'KNN Imputation': featuresKNN[column_name],\n",
        "     'Mean Imputation': featuresMean[column_name],\n",
        "     'Median Imputation': featuresMedian[column_name],\n",
        "     'MICE Imputation': featuresMICE[column_name],\n",
        "     'MostFrq Imputation': featuresMostFrq[column_name],\n",
        "     'MICE New Imputation': featuresMICE2[column_name]\n",
        "     }\n",
        "fdata = pd.DataFrame(data=d)\n",
        " \n",
        "fig, axes = plt.subplots(1, 7)\n",
        "fig.suptitle('Raw vs Imputed Desity Plots')\n",
        "fdata['raw'].plot.density(ax=axes[0], figsize = (15, 7))\n",
        "fdata['Mean Imputation'].plot.density(ax=axes[1], figsize = (15, 7), color='red')\n",
        "fdata['Median Imputation'].plot.density(ax=axes[2], figsize = (15, 7), color='red')\n",
        "fdata['MICE Imputation'].plot.density(ax=axes[3], figsize = (15, 7), color='red')\n",
        "fdata['MostFrq Imputation'].plot.density(ax=axes[4], figsize = (15, 7), color='red')\n",
        "fdata['KNN Imputation'].plot.density(ax=axes[5], figsize = (15, 7), color='red')\n",
        "fdata['MICE New Imputation'].plot.density(ax=axes[6], figsize = (15, 7), color='red')\n",
        "\n",
        "axes[0].set_title('Raw')\n",
        "axes[1].set_title('Mean Imputed')\n",
        "axes[2].set_title('Median Imputed')\n",
        "axes[3].set_title('MICE Imputed')\n",
        "axes[4].set_title('MostFrq Imputed')\n",
        "axes[5].set_title('KNN Imputed')\n",
        "axes[6].set_title('MICE New Imputation')\n",
        "\n",
        "axes[0].set_xlabel(column_name)\n",
        "axes[1].set_xlabel(column_name)\n",
        "axes[2].set_xlabel(column_name)\n",
        "axes[3].set_xlabel(column_name)\n",
        "axes[4].set_xlabel(column_name)\n",
        "axes[5].set_xlabel(column_name)\n",
        "axes[6].set_xlabel(column_name)"
      ],
      "metadata": {
        "id": "aJuBfUmdbsf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Outlier Identifcation"
      ],
      "metadata": {
        "id": "rlqeeuG7GT-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for df in imputed_features:\n",
        "  # split into train and test sets\n",
        "  X_train, X_test, y_train, y_test = train_test_split(df.iloc[:, 0:35], df.iloc[: , -1], test_size=0.33, random_state=1)\n",
        "  # summarize the shape of the training dataset\n",
        "  print('Original data shape: ' + str(X_train.shape) +  str(y_train.shape))\n",
        "  # identify outliers in the training dataset\n",
        "  iso = IsolationForest(contamination=0.1)\n",
        "  yhat = iso.fit_predict(X_train)\n",
        "  # select all rows that are not outliers\n",
        "  mask = yhat != -1\n",
        "  X_train, y_train = X_train.iloc[mask, :], y_train.iloc[mask]\n",
        "  # summarize the shape of the updated training dataset\n",
        "  print('Outliers removed: ' + str(X_train.shape) +  str(y_train.shape))"
      ],
      "metadata": {
        "id": "1TCxOnLEV3UX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SMOTE"
      ],
      "metadata": {
        "id": "2goFqQYCYBSr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check version number\n",
        "print(imblearn.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrkXiOHcYN-F",
        "outputId": "f3cd3d14-6dab-49a7-bb9e-8976c6f3247b"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize class distributions for each imputed data set\n",
        "for df in imputed_features:\n",
        "  counter = collections.Counter(df['response_3b'])\n",
        "  y = df['response_3b']\n",
        "  X = df.iloc[:, 0:35]\n",
        "  print(counter)"
      ],
      "metadata": {
        "id": "NTxm4sRLYS3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.core.multiarray import where\n",
        "# scatter plot of examples by class label\n",
        "for label, _ in counter.items():\n",
        "\trow_ix = where(y == label)[0]\n",
        "\tplt.scatter(X.iloc[row_ix, 0], X.iloc[row_ix, 1], label=str(label))\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7BJ57ZaLlQaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression"
      ],
      "metadata": {
        "id": "jVRfmq9F2NqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression model\n",
        "# Implement the model\n",
        "logreg = LogisticRegression()\n",
        "\n",
        "# hyperparameters, penalty values chosen to work with all solvers\n",
        "solvers = ['newton-cg', 'lbfgs', 'liblinear']\n",
        "penalty = ['l2']\n",
        "c_values = [100, 10, 1.0, 0.1, 0.01]\n",
        "\n",
        "# define grid search\n",
        "grid = dict(solver=solvers,penalty=penalty,C=c_values)\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "grid_search = GridSearchCV(estimator=logreg, param_grid=grid, n_jobs=-1, cv=cv, scoring='roc_auc',error_score=0)\n",
        "\n",
        "# SMOTE\n",
        "over = SMOTE(sampling_strategy=0.4)\n",
        "under = RandomUnderSampler(sampling_strategy=1)\n",
        "steps = [('o', over), ('u', under)]\n",
        "pipeline = Pipeline(steps=steps)\n",
        "\n",
        "for i in range(len(imputed_features)):\n",
        "  df = imputed_features[i]\n",
        "  X_train, X_test, y_train, y_test = train_test_split(df.iloc[:, 0:35], df['response_3b'], test_size=0.2, random_state=42)\n",
        "\n",
        "  # identify outliers in the training dataset\n",
        "  iso = IsolationForest(contamination=0.1)\n",
        "  yhat = iso.fit_predict(X_train)\n",
        "  # select all rows that are not outliers\n",
        "  mask = yhat != -1\n",
        "  X_train, y_train = X_train.iloc[mask, :], y_train.iloc[mask]\n",
        "  \n",
        "  X_train, y_train = pipeline.fit_resample(X_train, y_train)\n",
        "\n",
        "  grid_result = grid_search.fit(X_train, y_train)\n",
        "  grid_predict = grid_result.predict(X_test)\n",
        "\n",
        "  # summarize results\n",
        "  print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_), imputed_features_names[i])\n",
        "  print(confusion_matrix(y_test, grid_predict))\n",
        "  print(classification_report(y_test, grid_predict))"
      ],
      "metadata": {
        "id": "8NW-mkLdzB43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGBoost "
      ],
      "metadata": {
        "id": "RdtRB2_w29RB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Base XGBoost model"
      ],
      "metadata": {
        "id": "4eI9YuBCpbpS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Base XGBoost model\n",
        "\n",
        "# define model\n",
        "model = XGBClassifier()\n",
        "# define evaluation procedure\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "# evaluate model\n",
        "for i in range(len(imputed_features)):\n",
        "  df = imputed_features[i]\n",
        "  df_name = imputed_features_names[i]\n",
        "  X = df.iloc[:, 0:35]\n",
        "  y = df['response_3b']\n",
        "  scores = cross_val_score(model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
        "  # summarize performance\n",
        "  print('Mean ROC AUC: %.5f' % mean(scores))\n",
        "  print('For: ' + df_name)"
      ],
      "metadata": {
        "id": "JJsb5Gf2pPRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### XGBoost SMOTE"
      ],
      "metadata": {
        "id": "0VneeEN0krFn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SMOTE\n",
        "over = SMOTE(sampling_strategy=0.35)\n",
        "under = RandomUnderSampler(sampling_strategy=1)\n",
        "steps = [('o', over), ('u', under)]\n",
        "pipeline = Pipeline(steps=steps)\n",
        "\n",
        "# define model\n",
        "model = XGBClassifier()\n",
        "# define evaluation procedure\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "# evaluate model\n",
        "for i in range(len(imputed_features)):\n",
        "  df = imputed_features[i]\n",
        "  df_name = imputed_features_names[i]\n",
        "  X = df.iloc[:, 0:35]\n",
        "  y = df['response_3b']\n",
        "  X, y = pipeline.fit_resample(X, y)\n",
        "  scores = cross_val_score(model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
        "  # summarize performance\n",
        "  print('Mean ROC AUC: %.5f' % mean(scores))\n",
        "  print('For: ' + df_name)"
      ],
      "metadata": {
        "id": "KkoWxWuepGRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cost-Sensitive XGBoost"
      ],
      "metadata": {
        "id": "ClcNoJ5gkv3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# XGBoost documnetation suggests a good way to estimate scale_pos_weight = total_negative_examples / total_positive_examples\n",
        "estimate = 1869 / 7122\n",
        "\n",
        "# define model\n",
        "model = XGBClassifier(scale_pos_weight=estimate)\n",
        "# define evaluation procedure\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "# evaluate model\n",
        "for i in range(len(imputed_features)):\n",
        "  df = imputed_features[i]\n",
        "  df_name = imputed_features_names[i]\n",
        "  X = df.iloc[:, 0:35]\n",
        "  y = df['response_3b']\n",
        "  scores = cross_val_score(model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
        "  # summarize performance\n",
        "  print('Mean ROC AUC: %.5f' % mean(scores))\n",
        "  print('For: ' + df_name)\n"
      ],
      "metadata": {
        "id": "tiNlYovamQP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Same model as abouve with grid search for best scale_pos_weight value\n",
        "# define model\n",
        "model = XGBClassifier()\n",
        "# define grid\n",
        "weights = []\n",
        "for i in range(5):\n",
        "  weights.append(random.uniform(estimate, estimate*10))\n",
        "param_grid = dict(scale_pos_weight=weights)\n",
        "# define evaluation procedure\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "# define grid search\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=cv, scoring='roc_auc')\n",
        "# evaluate model\n",
        "for i in range(len(imputed_features)):\n",
        "  df = imputed_features[i]\n",
        "  df_name = imputed_features_names[i]\n",
        "  X = df.iloc[:, 0:35]\n",
        "  y = df['response_3b']\n",
        "  # execute the grid search\n",
        "  grid_result = grid.fit(X, y)\n",
        "  # report the best configuration\n",
        "  print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "  print('For: ' + df_name)"
      ],
      "metadata": {
        "id": "pjCMaRAftp8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final XGBoost Model"
      ],
      "metadata": {
        "id": "BOWoKjt236-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SMOTE\n",
        "over = SMOTE(sampling_strategy=0.35)\n",
        "under = RandomUnderSampler(sampling_strategy=1)\n",
        "steps = [('o', over), ('u', under)]\n",
        "pipeline = Pipeline(steps=steps)\n",
        "\n",
        "# define model\n",
        "model = XGBClassifier(scale_pos_weight=estimate)\n",
        "# define evaluation procedure\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "# evaluate model\n",
        "for i in range(len(imputed_features)):\n",
        "  df = imputed_features[i]\n",
        "  df_name = imputed_features_names[i]\n",
        "  X = df.iloc[:, 0:35]\n",
        "  y = df['response_3b']\n",
        "  X, y = pipeline.fit_resample(X, y)\n",
        "  counter = collections.Counter(y)\n",
        "  print(counter)\n",
        "  scores = cross_val_score(model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
        "  # summarize performance\n",
        "  print('Mean ROC AUC: %.5f' % mean(scores))\n",
        "  print('For: ' + df_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        },
        "id": "RUce7jke3-20",
        "outputId": "043dc2f9-0c1e-4bfd-d24f-848f6ec4cbdb"
      },
      "execution_count": 256,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({False: 2492, True: 2492})\n",
            "Mean ROC AUC: 0.73912\n",
            "For: featuresKNN\n",
            "Counter({False: 2492, True: 2492})\n",
            "Mean ROC AUC: 0.76754\n",
            "For: featuresMean\n",
            "Counter({False: 2492, True: 2492})\n",
            "Mean ROC AUC: 0.76527\n",
            "For: featuresMedian\n",
            "Counter({False: 2492, True: 2492})\n",
            "Mean ROC AUC: 0.74623\n",
            "For: featuresMICE\n",
            "Counter({False: 2492, True: 2492})\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-256-9b0f1c0c3177>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m   \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'roc_auc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m   \u001b[0;31m# summarize performance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mean ROC AUC: %.5f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m         \u001b[0merror_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m     )\n\u001b[1;32m    522\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test_score\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0merror_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         )\n\u001b[0;32m--> 283\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m     )\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 935\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    936\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}